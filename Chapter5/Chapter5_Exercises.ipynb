{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By: Ashutosh Singh\n",
    "- Email: singh.ashu@gmail.com\n",
    "- Reference: Advances in Financial Machine Learning, Marcos Lopez De Prado, pg 75\n",
    "\n",
    "\n",
    "# Fractionally Differentiated Features\n",
    "\n",
    "One of the challenges of quantitative analysis in finance is that price time series have trends or non-constant mean. This makes the time series non-stationary.  Non-stationary time series are hard to work with when we want to do inferential analysis such as average and variance of returns, or probability of loss.  Stationary series also help in supervised learning methods. Specifically, in supervised learning one needs to map hitherto unseen observations to a set of labeled examples and determine the label of the new observation.  As Marcos Lopez de Prado (MLdP) says in Chapter 5, “if the features are not stationary we cannot map the new observation to a large number of known examples”.  However, to make a time series (or a feature) stationary often requires data transformations like computing changes (change in price, yields or volatility).  These transformations also leave the time series bereft of any memory and thereby reducing or eliminating its predictive capability.  This chapter discusses ways to preserve as much memory as possible while transforming the series into a stationary time series. Or, proverbially “to have your cake and eat it too”.\n",
    "\n",
    "In this notebook we provide solutions to the exercies 5.1 through 5.6 and illustrate how fractionally differentiated series can be made stationary.  Exercises are particularly helpful in showing how to use fractionally differentaited series as a feature to train an algorithm.  Parts 5.6 (d) will be in a different notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import jarque_bera\n",
    "from scipy.stats import linregress\n",
    "from scipy.optimize import curve_fit \n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import coint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfinlab.filters import filters\n",
    "from mlfinlab.labeling import labeling\n",
    "from mlfinlab.util import utils\n",
    "from mlfinlab.features import fracdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = lambda s: adfuller(s, autolag='AIC')\n",
    "p_val = lambda s: adfuller(s, autolag='AIC')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Generate a time series from an IID Gaussian random process.  This is a memory-less, stationary process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 10000 IID random numbers \n",
    "nsample = 32768\n",
    "mu = 0\n",
    "sd = 1\n",
    "ts = np.random.normal(mu, sd, nsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the numbers\n",
    "plt.plot(ts)\n",
    "plt.title('IID Series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print various statistics of the time series\n",
    "print('Mean: {:.4f}, SD: {:.4f}, Skewness: {:.4f}, Kurtosis: {:.4f}'.format(np.mean(ts), np.std(ts), st.skew(ts), st.kurtosis(ts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1(a) Compute the ADF statistics on this series. What is the p-value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test = adfuller(ts, autolag='AIC')\n",
    "print(adf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The p-value is 0.0 and the abs(statistics) is very large compared to the critical value at 1% level, so we reject the null \n",
    "that there is a unit root*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 b: Compute the cumulative sum of the observations.  This is a non-stationary series with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum_ts = np.cumsum(ts)\n",
    "print(adf(cumsum_ts))\n",
    "print(p_val(cumsum_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The adfuller test shows that p-value (0.12) is not close to zero and the stat is not near the critical value of 1%.  Hence the series is not stationary as stated in the question.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.b.i Compute the order of integration of this cumulative series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that I(0) is not stationary (see above) ... Let's check I(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I(1)\n",
    "i1_series = np.ediff1d(cumsum_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1_series[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adf(i1_series))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The p-value is 0 indicating that the series is stationary*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 c: Differentiate the series twice. What is the p-value of this over-differentiated series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2_series = np.ediff1d(i1_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf(i2_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The p-value is 0.0 indicating that the series is stationary*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Generate a time series that follows a sinosoidal function. This is a stationary series with memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 100 points \n",
    "nsample = 1000\n",
    "\n",
    "## simulate a simple sinusoidal function\n",
    "x1 = np.linspace(0, 10, nsample)\n",
    "y = pd.Series(1*np.sin(2.0 * x1 + .5))\n",
    "y.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adf(y))\n",
    "print('\\np-value: {:0.6f}'.format(p_val(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The results indicate that the series is stationary*\n",
    "\n",
    "## 5.2 b: Shift every observation by the same positive value. Compute the cumulative sum of the observations. This is a non-stationary series with memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_constant = 1.\n",
    "y_shifted = (y + c_constant).cumsum().rename('Shifted_series').to_frame()\n",
    "y_shifted.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adf(y_shifted['Shifted_series']))\n",
    "print('\\np-value: {:0.6f}'.format(p_val(y_shifted['Shifted_series'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.ii Apply an expanding window fracDiff, with tau=1e-2. What is the minimum *d* value do you get a p-value below 5%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_series = fracdiff.frac_diff(y_shifted, 0.245)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf(fd_series['Shifted_series'].dropna())\n",
    "print('\\np-value: {:0.6f}'.format(p_val(fd_series['Shifted_series'].dropna())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The minimum value of d is 0.245*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.iii Apply FFD, with tau=1e-5. What is the minimum *d* value do you get a p-value below 5%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_series = fracdiff.frac_diff_ffd(y_shifted, 0.999999998999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf(fd_series['Shifted_series'].dropna())\n",
    "print('\\np-value: {:0.6f}'.format(p_val(fd_series['Shifted_series'].dropna())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The minimum value of d is 0.999999998999*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Take the series from exercise 5.2.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Fit the series to a sine function. What is the R-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the series from 5.2.b - which is teh cumulative series (y_shifted)\n",
    "y_shifted.plot(figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x1\n",
    "y_data = y_shifted['Shifted_series'].values\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from: https://stackoverflow.com/questions/16716302/how-do-i-fit-a-sine-curve-to-my-data-with-pylab-and-numpy/42322656"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sin(tt, yy):\n",
    "    ''' Fit sin to the input time sequence, and return fitting parameters \n",
    "    \"amp\", \"omega\", \"phase\", \"offset\", \"freq\", \"period\" and \"fitfunc\"\n",
    "    '''\n",
    "    tt = np.array(tt)\n",
    "    yy = np.array(yy)\n",
    "    ff = np.fft.fftfreq(len(tt), (tt[1]-tt[0]))   # assume uniform spacing\n",
    "    Fyy = abs(np.fft.fft(yy))\n",
    "    guess_freq = abs(ff[np.argmax(Fyy[1:])+1])   # excluding the zero frequency \"peak\", which is related to offset\n",
    "    guess_amp = np.std(yy) * 2.**0.5\n",
    "    guess_offset = np.mean(yy)\n",
    "    guess = np.array([guess_amp, 2.*np.pi*guess_freq, 0., guess_offset])\n",
    "\n",
    "    def sinfunc(t, A, w, p, c):  return A * np.sin(w*t + p) + c\n",
    "    \n",
    "    popt, pcov = curve_fit(sinfunc, tt, yy, p0=guess)\n",
    "    A, w, p, c = popt\n",
    "    f = w/(2.*np.pi)\n",
    "    fitfunc = lambda t: A * np.sin(w*t + p) + c\n",
    "    \n",
    "    return {\"amp\": A, \"omega\": w, \"phase\": p, \"offset\": c, \"freq\": f, \"period\": 1./f, \n",
    "            \"fitfunc\": fitfunc, \"maxcov\": np.max(pcov), \"rawres\": (guess,popt,pcov)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fit_sin(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Amplitude=%(amp)s, Angular freq.=%(omega)s, phase=%(phase)s, \\noffset=%(offset)s, Max. Cov.=%(maxcov)s\" % res )\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x_data, y_data, \"-k\", label=\"y\", linewidth=2)\n",
    "plt.plot(x_data, res[\"fitfunc\"](x_data), \"r-\", label=\"y fit curve\", linewidth=2)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearly regress the computed values with those of the original values and extract the r-squared\n",
    "computed_y_values = res['fitfunc'](x_data)\n",
    "slope, intercept, r_val, _, _ = linregress(y_data, computed_y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Slope: {:0.3f}, intercept: {:0.3f}, r-sq: {:0.3f}'.format(slope, intercept, r_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.b. Apply FFD(d=1). Fit the series to a sine function. What is the r-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_series = fracdiff.frac_diff_ffd(y_shifted, diff_amt=1.0).dropna()\n",
    "# fd_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = fd_series.index\n",
    "y_data = fd_series['Shifted_series'].values\n",
    "res = fit_sin(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Amplitude=%(amp)s, Angular freq.=%(omega)s, phase=%(phase)s, \\noffset=%(offset)s, Max. Cov.=%(maxcov)s\" % res )\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x_data, y_data, \"-k\", label=\"y\", linewidth=2)\n",
    "plt.plot(x_data, res[\"fitfunc\"](x_data), \"r-\", label=\"y fit curve\", linewidth=2)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearly regress the computed values with those of the original values and extract the r-squared\n",
    "computed_y_values = res['fitfunc'](x_data)\n",
    "slope, intercept, r_val, _, _ = linregress(y_data, computed_y_values)\n",
    "print('Slope: {:0.3f}, intercept: {:0.3f}, r-sq: {:0.10f}'.format(slope, intercept, r_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.c What value of *d* maximizes the r-squared of a sinusoidal fit on FFD(d)? why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The r-squared is 1.00 (that is the maximum value). fracDiff_FFD with d=1 makes the series stationary (there is no trend and it is homoscadestic) and since there is no noise the curve fit is complete.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Take the dollar bar series on E-mini S&P 500 futures. Using the code in Snippet 5.3, for some d in [0, 2], compute the fracDiff_FFD(fracDiff_FFD(series, d), -d). What do you get? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data = pd.read_csv('../Sample-Data/dollar_bars.csv')\n",
    "data.index = pd.to_datetime(data['date_time'])\n",
    "data = data.drop('date_time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_series = data['close'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1.5\n",
    "fd = fracdiff.frac_diff_ffd(fracdiff.frac_diff_ffd(data_series, d), -d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The result is that the series fd is all NaNs.  The reason is when d is negative it takes an almost infinite number of observations for the weights to be lower than the threshold. That in turn leads to all the elements of the \"seriesF\" in frac_diff_ffd to have a value of NaNs. See the example below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 0.9\n",
    "w = fracdiff.get_weights_ffd(d, 1e-2, 10)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = fracdiff.get_weights_ffd(-d, 1e-2, 10)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 Take the dollar bar series on E-mini S&P 500 futures. \n",
    "\n",
    "## 5.5.a Form a new series as a cumulative sum of log prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data = pd.read_csv('../Sample-Data/dollar_bars.csv')\n",
    "data.index = pd.to_datetime(data['date_time'])\n",
    "data = data.drop('date_time', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the close prices\n",
    "data_series = data['close'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form the cumulative sum of the log prices\n",
    "log_prices = np.log(data_series).cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.b Apply FFD, with tau = 1e-5. Determine for what d in [0, 2] the series is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1.999999979\n",
    "fd_series = fracdiff.frac_diff_ffd(log_prices, diff_amt=d, thresh=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val(fd_series['close'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_min_ffd(close_prices):\n",
    "    out = pd.DataFrame(columns=['adfStat', 'pVal', 'lags', 'nObs', '95% conf', 'corr'])\n",
    "    for d in np.linspace(0, 1, 11):\n",
    "        df1 = np.log(close_prices[['close']]).resample('1D').last()  # downcast to daily obs        \n",
    "        df1.dropna(inplace=True)\n",
    "        df2 = fracdiff.frac_diff_ffd(df1, diff_amt=d, thresh=0.01).dropna()\n",
    "        corr = np.corrcoef(df1.loc[df2.index, 'close'], df2['close'])[0, 1]\n",
    "        df2 = adfuller(df2['close'], maxlag=1, regression='c', autolag=None)\n",
    "        out.loc[d] = list(df2[:4]) + [df2[4]['5%']] + [corr]  # with critical value\n",
    "    out[['adfStat', 'corr']].plot(secondary_y='adfStat', figsize=(10, 8))\n",
    "    plt.axhline(out['95% conf'].mean(), linewidth=1, color='r', linestyle='dotted')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_min_ffd(data_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.c Compute the correlation of fracdiff series to the original (untransformed) series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = log_prices.copy().rename(columns={'close':'cumsum'})\n",
    "comb_df = comb_df.join(fd_series.rename(columns={'close':'fdseries'})).dropna()\n",
    "comb_df = comb_df.join(data_series.rename(columns={'close':'original'})).dropna()\n",
    "comb_df.head()\n",
    "comb_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.d Apply an Engle-Granger cointegration test on the original and the fracdiff series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df['original'].plot()\n",
    "ax = comb_df['fdseries'].plot(secondary_y=True, color='r')\n",
    "ax.set_ylabel('FD Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_prices = comb_df['original'].ravel()\n",
    "fd_prices = comb_df['fdseries'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = coint(cl_prices, fd_prices, autolag='AIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('P-value: {:.6f}'.format(res[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The p-value is below the critical value of 0.05, so we can reject the NULL that there is no cointegration.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.e Apply a Jarque-Bera normality test on the fracdiff series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jb_test = jarque_bera(fd_prices)\n",
    "print('P-value: {:.6f}'.format(jb_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The p-value is below 0.05 critical value, so we can reject the NULL hypothesis \n",
    "that data has skewness and kurtosis matching normal distribution.  This shows that \n",
    "the underlying distribution of fd_prices is not Gaussian.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.6 Take the the fracDiff series from 5.5 \n",
    "\n",
    "## 5.6.a Apply a CUSUM filter (Chapter 2), where h is twice the standard deviation of the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 23:00:23.723</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 07:07:35.156</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 09:35:57.204</th>\n",
       "      <td>-0.001697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 12:59:42.176</th>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02 14:19:33.847</th>\n",
       "      <td>0.001333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            close\n",
       "date_time                        \n",
       "2015-01-01 23:00:23.723       NaN\n",
       "2015-01-02 07:07:35.156       NaN\n",
       "2015-01-02 09:35:57.204 -0.001697\n",
       "2015-01-02 12:59:42.176  0.000243\n",
       "2015-01-02 14:19:33.847  0.001333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we repeat the steps of 5.5 to get the fracDiff series\n",
    "# Read in data\n",
    "data = pd.read_csv('../Sample-Data/dollar_bars.csv')\n",
    "data.index = pd.to_datetime(data['date_time'])\n",
    "data = data.drop('date_time', axis=1)\n",
    "\n",
    "# get the close prices\n",
    "data_series = data['close'].to_frame()\n",
    "\n",
    "# form the cumulative sum of the log prices\n",
    "log_prices = np.log(data_series).cumsum()\n",
    "\n",
    "# Compute the fracDiff_FFD\n",
    "d = 2.0\n",
    "fd_series = fracdiff.frac_diff_ffd(log_prices, diff_amt=d, thresh=1e-5)\n",
    "fd_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute volatility\n",
    "vol = fd_series.std()\n",
    "print('Volatility: {:0.4f}'.format(vol[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cusum filter\n",
    "cusum_events = filters.cusum_filter(fd_series.dropna(), threshold=vol[0]*2.)\n",
    "cusum_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.b Use the filtered timestamps to sample a features' matrix. Use as one of the features the fracDiff value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_strength_index(df, n):\n",
    "    \"\"\" Calculate Relative Strength Index(RSI) for given data.\n",
    "    https://github.com/Crypto-toolbox/pandas-technical-indicators/blob/master/technical_indicators.py\n",
    "\n",
    "    :param df: pandas.DataFrame\n",
    "    :param n: \n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    UpI = [0]\n",
    "    DoI = [0]\n",
    "    while i + 1 <= df.index[-1]:\n",
    "        UpMove = df.loc[i + 1, 'high'] - df.loc[i, 'high']\n",
    "        DoMove = df.loc[i, 'low'] - df.loc[i + 1, 'low']\n",
    "        if UpMove > DoMove and UpMove > 0:\n",
    "            UpD = UpMove\n",
    "        else:\n",
    "            UpD = 0\n",
    "        UpI.append(UpD)\n",
    "        if DoMove > UpMove and DoMove > 0:\n",
    "            DoD = DoMove\n",
    "        else:\n",
    "            DoD = 0\n",
    "        DoI.append(DoD)\n",
    "        i = i + 1\n",
    "    UpI = pd.Series(UpI)\n",
    "    DoI = pd.Series(DoI)\n",
    "    PosDI = pd.Series(UpI.ewm(span=n, min_periods=n).mean())\n",
    "    NegDI = pd.Series(DoI.ewm(span=n, min_periods=n).mean())\n",
    "    RSI = pd.Series(round(PosDI * 100. / (PosDI + NegDI)), name='RSI_' + str(n))\n",
    "    # df = df.join(RSI)\n",
    "    return RSI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RSI\n",
    "def get_rsi(data, window=14):\n",
    "    df = data.copy(deep=True).reset_index()\n",
    "    rsi = relative_strength_index(df, window)\n",
    "    rsi_df = pd.Series(data=rsi.values, index=data.index)\n",
    "    return rsi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsi_df = get_rsi(data, window=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsi_df.head(20)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dol_bars_feature = data['close'].loc[cusum_events]\n",
    "frac_diff_feature = fd_series.loc[cusum_events]\n",
    "rsi_feature = rsi_df[cusum_events]\n",
    "\n",
    "features_mat = (pd.DataFrame()\n",
    "                .assign(dollar_bars=dol_bars_feature,\n",
    "                        frac_diff=frac_diff_feature,\n",
    "                        rsi=rsi_feature)\n",
    "                .drop_duplicates().dropna())\n",
    "\n",
    "features_mat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.c Form labels using the triple-barrier method, with symmetric horizontal barriers of twice the daily standard deviation, and a vertical barrier of 5 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute daily volatility\n",
    "daily_vol = utils.get_daily_vol(features_mat.dollar_bars)\n",
    "\n",
    "# compute vertical barriers\n",
    "# t1 = snp.addVerticalBarrier(tEvents, ftMtx.dbars, numDays=5)\n",
    "vertical_barriers = labeling.add_vertical_barrier(t_events=cusum_events, close=features_mat.dollar_bars, num_days=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triple barrier\n",
    "pt_sl = [2, 2]\n",
    "min_ret = 0.0005\n",
    "triple_barrier_events = labeling.get_events(close=features_mat.dollar_bars,\n",
    "                                  t_events=cusum_events,\n",
    "                                  pt_sl=pt_sl,\n",
    "                                  target=daily_vol,\n",
    "                                  min_ret=min_ret,\n",
    "                                  num_threads=2,\n",
    "                                  vertical_barrier_times=vertical_barriers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_barrier_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels\n",
    "labels = labeling.get_bins(triple_barrier_events, features_mat.dollar_bars)\n",
    "clean_labels = labeling.drop_labels(labels)\n",
    "clean_labels.bin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
