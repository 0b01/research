{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By: Illya Barziy\n",
    "* Email: illyabarziy@gmail.com\n",
    "* Reference: __Backtesting__ _by_ Campbell R. Harvey _and_ Yan Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haircut Sharpe Ratios and Profit Hurdle algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A General overview of the framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a common practice to discount reported Sharpe ratios by 50% as a result of data mining. The authors of the research developed an analytical way to determine the haircut's magnitude. \n",
    "\n",
    "The haircut is the percentage difference between the original Sharpe ratio and the Sharpe ratio adjusted to the effect of data mining.\n",
    "\n",
    "The authors explain that their framework relies on the concept of multiple testing. \n",
    "- If a a set of data $X$ explains $Y$ and the relation is significant with t-ratio of 2.0 (it has a probability value of 0.05), we refer to it as a single test. \n",
    "- If multiple sets of data $X_1, X_2, .., X_n$ explain $Y$, the same criteria for significance cannot be used (Some of the variables can produce t-ratios 2.0 and higher). Then, what is the appropriate cut off for statistical significance? \n",
    "\n",
    "Generally speaking, with a higher number of sets, the t-ratio is also higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a strategy produces a Sharpe ratio, it's transformed into a t-ratio and then to p-value that takes into account multiple testing. \n",
    "\n",
    "In order to use the framework, one has to decide on the number of previous tests. In the research of _Harvey, C.R._, _Y. Liu_, and _H. Zhu_ __“… and the Cross-section of\n",
    "Expected Returns.”__  [available here](https://faculty.fuqua.duke.edu/~charvey/Research/Published_Papers/P118_and_the_cross.PDF) at least 316 factors explaining the cross-sectional patterns in equity returns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the provided approach, the haircut is nonlinear and the marginal Sharpe ratios are heavily penalized in comparison to high Sharpe ratios. Researchers state that it has economic sense, as strategies with high Sharpe ratios have a higher probability of being true discoveries.\n",
    "\n",
    "Researchers point to the following caveatas of the method:\n",
    "- High Sharpe ratios may be a result of non-normal distribution of returns. Therefore, Sharpe ratios should be viewed in the context fo the distribution of returns.\n",
    "- Sharpe ratios are not the only measures of risk, hovever the approach also applies to information ratios.\n",
    "- Need for determining the significance level for multiple testing.\n",
    "- Need to choose between the adjustment methods used in the framework provided (there are four of them).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $r_t$ denote the return for an investment strategy between time $t-1$ and $t$. The strategy can consist of returns from both long and short positions. \n",
    "\n",
    "In order to conclude if the strategy is able to maintain true profits, a statistical test is formed to see if the expected excess returns are different from zero. \n",
    "\n",
    "Given a set of returns $(r_1, r_2, .., r_T)$, we denote $\\mu$ as the mean and $\\sigma$ as the standard deviation. T-statistic to test the null-hypothesis that the average return is zero is:\n",
    "\n",
    "$t-statistic = \\frac{\\mu}{\\sigma/\\sqrt{T}}$\n",
    "\n",
    "_The returns are assumed to be i.i.d. normal_, then the described t-statistic follows a t-distribution with $T-1$ degrees of freedom. This way we can assess the statistical significance of the investment strategy. \n",
    "\n",
    "At the same time, the Sharpe ratio is defined as:\n",
    "\n",
    "$SR = \\frac{\\mu}{\\sigma}$\n",
    "\n",
    "Therefore, based on the previous equation, \n",
    "\n",
    "$SR = \\frac{t-ratio}{\\sqrt{T}}$\n",
    "\n",
    "This shows that a higher Sharpe ratio implies higher t-statistic, which implies higher significance level (with fixed T).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To adjust the Sharpe ratio for data mining bias, first we calculate the p-value of a single test:\n",
    "\n",
    "$p^s = Pr(|r|>t-ratio) = Pr(|r|>SR*\\sqrt{T})$, where $r$ is a random variable of a t-sistribution.\n",
    "\n",
    "This metric doesn't make sense when hundreds of strategies were tested only the most profitable is presented. \n",
    "\n",
    "If N strategies were tested (and we assume the test statistics for N strategies to be independent), under the null hypothesis that none of the strategies can generate non-zero returns, multiple testing p-value is:\n",
    "\n",
    "$p^M = Pr(max\\{|r_i|, i = 1, .., N\\}>t-ratio) = $\n",
    "\n",
    "$= 1 - \\prod^N_{i=1}Pr(|r_i|\\le t-ratio) = 1 - (1 - p^S)^N$\n",
    "\n",
    "For $N=10, p^S=0.05$ whereas $p^M=0.401$. Multiple testing greatly reduces the statistical significance of a single test. \n",
    "\n",
    "Equating the p-value of a single test to $p^M$ will provide the equation for calculating the adjusted (haircut) Sharpe ratio $HSR$:\n",
    "\n",
    "$p^M = Pr(|r|>HSR * \\sqrt{T})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A numerical example:__ \n",
    "\n",
    "$T = 200$ - ten years of monthly observations, $SR = 0.75$ - observed annual Sharpe ratio of $0.75$ and p-value of $0.0008$ in a single test.  When we assume the number of other strategies tested $N = 200$ and, therefore $p^M = 0.15$, we can calculate the adjusted Sharpe ratio $HSR = 0.32$, thus being reduced by $60\\%$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation is true when N strategies are independent, however, this approach is not applicable for real-life cases. For this reason, in the paper _Harvey, C.R._, _Y. Liu_, and _H. Zhu_ __“… and the Cross-section of\n",
    "Expected Returns.”__  [available here](https://faculty.fuqua.duke.edu/~charvey/Research/Published_Papers/P118_and_the_cross.PDF) authors provide a multiple testing framework to find the appropriate p-value adjustment. This model is referred to as the HLZ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The HLZ model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model adjusts p-values for multiple testing taking into account that the strategies are not independent. It consists of three methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonferroni method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the p-values are ordered in ascending orders.\n",
    "\n",
    "$p_{(1)} \\le p_{(2)} \\le ... \\le p_{(M)}$\n",
    "\n",
    "This method adjusts each p-value equally - inflates the original p-value by the number of tests $M$:\n",
    "\n",
    "$p^{Bonferroni}_{(i)} = min[M*p_{(i)}, 1], i=1, .., M$\n",
    "\n",
    "__A numerical example:__\n",
    "\n",
    "We observe $M = 6$ strategies with an ordered sequence of p-values of $(0.005, 0.009, 0.0128, 0.135, 0.045, 0.06)$. Under the single tests, five of the strategies are significant. Under the Bonferroni adjustment, however, p-values are $(0.03, 0.054, 0.0768, 0.081, 0.27, 0.36)$, which makes only the first strategy significant under Bonferroni method adjustments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holm method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same ordered set of p-values:\n",
    "\n",
    "$p_{(1)} \\le p_{(2)} \\le ... \\le p_{(M)}$\n",
    "\n",
    "The Holm method with $M$ tests adjusts p-values as follows:\n",
    "\n",
    "$p^{Holm}_{(i)} = min[max_{j \\le i} \\{(M-j+1)*p_{(j)}, 1], i = 1, .., M$\n",
    "\n",
    "The Holm method starts with the smallest p-value.\n",
    "\n",
    "__A numerical example:__\n",
    "\n",
    "Using the set of p-values from the previous example ($M = 6$,  $(0.005, 0.009, 0.0128, 0.135, 0.045, 0.06)$):\n",
    "\n",
    "$p^{Holm}_{(1)} = 6 * p_{(1)} = 0.03$\n",
    "\n",
    "$p^{Holm}_{(2)} = max[6p_{(1)}, 5p_{(2)}] = 0.045$\n",
    "\n",
    "$p^{Holm}_{(3)} = max[6p_{(1)}, 5p_{(2)}, 4p_{(3)}] = 0.0512$\n",
    "\n",
    "$p^{Holm}_{(4)} = max[6p_{(1)}, 5p_{(2)}, 4p_{(3)}, 3p_{(4)}] = 0.09$\n",
    "\n",
    "$p^{Holm}_{(5)} = max[6p_{(1)}, 5p_{(2)}, 4p_{(3)}, 3p_{(4)}, 2p_{(5)}] = 0.09$\n",
    "\n",
    "$p^{Holm}_{(6)} = max[6p_{(1)}, 5p_{(2)}, 4p_{(3)}, 3p_{(4)}, 2p_{(5)}, p_{(6)}] = 0.09$\n",
    "\n",
    "Which makes the first two strategies significant under the Holm method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benjamini, Hochberg and Yekutieli (BHY) method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same ordered set of p-values:\n",
    "\n",
    "$p_{(1)} \\le p_{(2)} \\le ... \\le p_{(M)}$\n",
    "\n",
    "Starting from the highest p-value, the adjusted values are calculated as:\n",
    "\n",
    "$ p^{BHY}_{(i)} =\n",
    "  \\begin{cases}\n",
    "    p_{(M)}       & \\quad \\text{if } i=M \\text{,}\\\\\n",
    "    min[p^{BHY}_{(i)}, \\frac{M*c(M)}{i}p_{(i)}]  & \\quad \\text{if } i \\le M-1 \\text{,}\n",
    "  \\end{cases}$\n",
    "  \n",
    "where $c(M) = \\sum^{M}_{j=1}\\frac{1}{j}$.\n",
    "\n",
    "Starting from the highest p-value, the method moves towards the smallest through pairwise comparison. \n",
    "\n",
    "__A numerical example:__\n",
    "\n",
    "Using the set of p-values from the previous example ($M = 6$,  $(0.005, 0.009, 0.0128, 0.135, 0.045, 0.06)$):\n",
    "\n",
    "$c(M) = \\sum^{6}_{j=1}\\frac{1}{j} = 2.45$\n",
    "\n",
    "$p^{BHY}_{(6)} = p_{(6)} = 0.06$\n",
    "\n",
    "$p^{BHY}_{(5)} = min[p^{BHY}_{(6)}, \\frac{6*2.45}{5}p_{(5)}] = p^{BHY}_{(6)} = 0.06$\n",
    "\n",
    "$p^{BHY}_{(4)} = min[p^{BHY}_{(5)}, \\frac{6*2.45}{4}p_{(4)}] = \\frac{6*2.45}{4}p_{(4)} = 0.0496$\n",
    "\n",
    "$p^{BHY}_{(3)} = min[p^{BHY}_{(4)}, \\frac{6*2.45}{3}p_{(3)}] = p^{BHY}_{(4)} = 0.0496$\n",
    "\n",
    "$p^{BHY}_{(2)} = min[p^{BHY}_{(3)}, \\frac{6*2.45}{2}p_{(2)}] = p^{BHY}_{(3)} = 0.0496$\n",
    "\n",
    "$p^{BHY}_{(1)} = min[p^{BHY}_{(2)}, \\frac{6*2.45}{1}p_{(1)}] = p^{BHY}_{(2)} = 0.0496$\n",
    "\n",
    "Which makes the first four strategies significant under the BHY method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of the methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the Bonferroni and Holm methods, \n",
    "\n",
    "$p^{Holm}_{(i)} \\le p^{Bonferrroni}_{(i)}$ for any $i$\n",
    "\n",
    "Making Bonferroni method a tougher one comparing to Holm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlfinlab",
   "language": "python",
   "name": "mlfinlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
