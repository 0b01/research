{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By: Jacques Joubert\n",
    "* Email: jacques@quantsportal.com\n",
    "* Reference: Advances in Financial Machine Learning, Marcos Lopez De Prado, pg 101\n",
    "\n",
    "\n",
    "# Chapter 6 Ensemble Methods\n",
    "\n",
    "## Question 1:\n",
    "**Why is bagging based on random sampling with replacement? Would bagging still reduce a forecast’s variance if sampling were without replacement?**\n",
    "\n",
    "Sampling without replacement is called Pasting and with replacement is Bagging. Pasting is designed for very large data sets that can afford to make use of sampling without replacement. Both have the same purpose in mind - to create a diverse set of models. These diverse models are then used in an ensemble method which has a similar bias but a lower variance than a single predictor trained on the original training set.\n",
    "\n",
    "Bagging ends up with a slightly higher bias than pasting which results in the predictors being less correlated, thus the ensemble's variance is reduced. Overall bagging is preferred as it often leads to better models.\n",
    "\n",
    "## Question 2:\n",
    "**Suppose that your training set is based on highly overlapping labels (i.e., with low uniqueness, as defined in Chapter 4).**\n",
    "\n",
    "**(a) Does this make bagging prone to overfitting, or just ineffective? Why?**\n",
    "\n",
    "“Redundant observations have two detrimental effects on bagging. First Samples drawn with replacement are more likely to be virtually identical, even if they do not share the same observations. This makes p_bar ≈ 1, and bagging will not reduce variance, regardless of the number of estimators N.”\n",
    "\n",
    "The advantage of using Bagging lays in its ability to reduce forecast variance and thus prevents overfitting. The variance of the bagged prediction is a function of the number of bagged estimators, the average variance of a single estimator’s prediction, and the average correlation among their forecasts.\n",
    "\n",
    "Models that are trained on the same type of data are likely to make the same type of errors. When there are many overlapping samples (low uniqueness) then it results in models with poor diversity (high correlation).\n",
    "\n",
    "Bagging is only effective to the extent that the average correlation among forecasters is less than 1. One of the goals of sequential bootstrapping (Chapter 4) is to produce samples as independent as possible, thereby reducing the avg correlation, which should lower the variance of bagging classifiers.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/hudson-and-thames/research/ch6_questions/Chapter6/images/ch6_bagged_prediction.png\" width=\"500\"><br>\n",
    "</div>\n",
    "\n",
    "\n",
    "The plot above shows that models that are highly correlated (overbar p), fail to reduce the variance. It is better to have a few diverse models than a large number of correlated ones.\n",
    "\n",
    "Thus training models on data that has a low average uniqueness, makes the bagging ensemble more ineffective. It also leads to the out-of-bag accuracy being grossly over-inflated.\n",
    "\n",
    "**(b) Is out-of-bag accuracy generally reliable in financial applications? Why?**\n",
    "\n",
    "“The second detrimental effect from observation redundancy is that out-of-bag accuracy will be inflated. This happens because random sampling with replacement places in the training set samples that are very similar to those out-of-bag. In such a case, a proper stratified k-fold cross-validation without shuffling before partitioning will show a much lower testing-set accuracy than the one estimated out-of-bag. For this reason, it is advisable to set StratifiedKFold(n_splits=k, shuffle=False) when using that sklearn class, cross-validate the bagging classifier, and ignore the out-of-bag accuracy results. A low number k is preferred to a high one, as excessive partitioning would again place in the testing set samples too similar to those used in the training set.”\n",
    "\n",
    "\n",
    "## Question 3:\n",
    "\n",
    "**Build an ensemble of estimators, where the base estimator is a decision tree.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris, make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data\n",
    "X, y = make_classification(n_samples=2000, n_features=30,\n",
    "                            n_informative=15, n_redundant=10,\n",
    "                            random_state=42, shuffle=True)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-bag score on training: 0.8471428571428572\n",
      "Test Score: 0.8566666666666667\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter=\"best\", max_leaf_nodes=None),\n",
    "                            n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "# Fit and score\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "print('Out-of-bag score on training: {}'.format(bag_clf.oob_score_))\n",
    "print('Test Score: {}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) How is this ensemble different from an RF?**\n",
    "\n",
    "A random forest also makes use of a bagging method and thus it has the same hyperparameters as a decsion tree and a bagging classifier. However the key difference is that it introduces extra randomness when building trees as it splits the data on the best feature from a random subset of features. This results in greater tree diversity, thus lowering the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Using sklearn, produce a bagging classifier that behaves like an RF. What parameters did you have to set up, and how?**\n",
    "\n",
    "First we build a random forest classifier and then we fit a bagging classifier which has its parameters adjusted to replicate the random forrest. In order to do this, we need to set the splitter hyperparameter in the decision trees to random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, oob_score=True)\n",
    "rnd_clf.fit(X_train, y_train) \n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "\n",
    "# Bagging -> RF\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "                            n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred_bagging = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier\n",
      "Out-of-bag score on training: 0.8707142857142857\n",
      "Test Score: 0.87\n",
      "\n",
      "Bagging Classifier\n",
      "Out-of-bag score on training: 0.8635714285714285\n",
      "Test Score: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest Classifier')\n",
    "print('Out-of-bag score on training: {}'.format(rnd_clf.oob_score_))\n",
    "print('Test Score: {}'.format(accuracy_score(y_test, y_pred_rf)))\n",
    "print('')\n",
    "\n",
    "print('Bagging Classifier')\n",
    "print('Out-of-bag score on training: {}'.format(bag_clf.oob_score_))\n",
    "print('Test Score: {}'.format(accuracy_score(y_test, y_pred_bagging)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sandbox\n",
    "\n",
    "1. Set a parameter max_features to a lower value, as a way of forcing discrepancy between trees.\n",
    "2. Early stopping: Set the regularization parameter min_weight_fraction_ leaf to a sufficiently large value (e.g., 5%) such that out-of-bag accuracy converges to out-of-sample (k-fold) accuracy.\n",
    "3. Use BaggingClassifier on DecisionTreeClassifier where max_samples is set to the average uniqueness (avgU) between samples. \n",
    "    * (a) clf = DecisionTreeClassifier(criterion='entropy', max_features='auto', class_weight='balanced')\n",
    "    * (b) bc = BaggingClassifier(base_estimator=clf, n_estimators=1000, max_samples=avgU, max_features=1.)\n",
    "4. Use BaggingClassifier on RandomForestClassifier where max_samples is set to the average uniqueness (avgU) between samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier\n",
      "Out-of-bag score on training: 0.8221428571428572\n",
      "Test Score: 0.8383333333333334\n"
     ]
    }
   ],
   "source": [
    "# Bagging -> RF\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, min_weight_fraction_leaf=0.05),\n",
    "                            n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, oob_score=True, \n",
    "                           max_features=0.8)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred_bagging = bag_clf.predict(X_test)\n",
    "\n",
    "print('Bagging Classifier')\n",
    "print('Out-of-bag score on training: {}'.format(bag_clf.oob_score_))\n",
    "print('Test Score: {}'.format(accuracy_score(y_test, y_pred_bagging)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfinlab.sampling import get_ind_mat_average_uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mget_ind_mat_average_uniqueness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Snippet 4.4. page 65, Compute Average Uniqueness\n",
       "Average uniqueness from indicator matrix\n",
       "\n",
       ":param ind_mat: (np.matrix) indicator binary matrix\n",
       ":return: (np.matrix) matrix with label uniqueness\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/mlfinlab/lib/python3.6/site-packages/mlfinlab/sampling/bootstrapping.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_ind_mat_average_uniqueness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
